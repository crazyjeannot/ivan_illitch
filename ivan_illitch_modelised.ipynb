{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import timeit\n",
    "import math\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from os import path\n",
    "from collections import Counter\n",
    "from lxml import etree\n",
    "from glob import glob\n",
    "from unicodedata import normalize\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('fr_core_news_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.pipeline.sentencizer.Sentencizer at 0x7fae5dd37a40>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.add_pipe(\"sentencizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(txt):\n",
    "    txt_res = normalize(\"NFKD\", str(txt).replace(u'\\xa0', u' '))\n",
    "    txt_res = txt_res.replace(u'\\\\xa0', u' ')\n",
    "    txt_res = txt_res.replace(u'\\\\n', u'')\n",
    "    txt_res = txt_res.replace(u'\\n', u'')\n",
    "    txt_res = txt_res.replace(u'\\\\', u'')\n",
    "    txt_res = txt_res.replace(u'\\'', u'')\n",
    "\n",
    "    return txt_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_most_common_features(list_tokens, n):\n",
    "    table_freq = dict(Counter(list_tokens).most_common(n))\n",
    "    return list(table_freq.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigrammize(list_token):\n",
    "    \"\"\"fonction qui prend en parametre une liste de tokens et retourne une liste de bi-grammes\"\"\"\n",
    "    list_bigram = []\n",
    "    for indice_token in range(len(list_token)-1):\n",
    "        bigram = list_token[indice_token]+'_'+list_token[indice_token+1]\n",
    "        list_bigram.append(bigram)\n",
    "    return list_bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trigrammize(list_token):\n",
    "    \"\"\"fonction qui prend en parametre une liste de tokens et retourne une liste de tri-grammes\"\"\"\n",
    "    list_trigram = []\n",
    "    for indice_token in range(len(list_token)-2):\n",
    "        trigram = list_token[indice_token]+'_'+list_token[indice_token+1]+'_'+list_token[indice_token+2]\n",
    "        list_trigram.append(trigram)\n",
    "    return list_trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_features(path_name, n):\n",
    "    str_base = \" \"\n",
    "    list_illitch = []\n",
    "    for doc in glob(path_name):\n",
    "        list_lemma, list_token, nombre_tokens = pipeline_spacy(doc)\n",
    "        print(doc +' : '+str(nombre_tokens)+' tokens')\n",
    "        list_bigram_lemma = bigrammize(list_lemma)\n",
    "        list_trigram_lemma = trigrammize(list_lemma)\n",
    "        \n",
    "        list_bigram_token = bigrammize(list_token)\n",
    "        list_trigram_token = trigrammize(list_token)\n",
    "        \n",
    "        \n",
    "        list_lemma_result = get_n_most_common_features(list_lemma, n)\n",
    "        list_bigram_lemma_result = get_n_most_common_features(list_bigram_lemma, n)\n",
    "        list_trigram_lemma_result = get_n_most_common_features(list_trigram_lemma, n)\n",
    "    \n",
    "        list_token_result = get_n_most_common_features(list_token, n)\n",
    "        list_bigram_token_result = get_n_most_common_features(list_bigram_token, n)\n",
    "        list_trigram_token_result = get_n_most_common_features(list_trigram_token, n)\n",
    "\n",
    "    return list_lemma_result, list_bigram_lemma_result, list_trigram_lemma_result, list_token_result, list_bigram_token_result, list_trigram_token_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunkise(list_token, n):\n",
    "    list_chunks = []\n",
    "    for i in range(0, len(list_token)-n, n):\n",
    "        list_chunks.append(list_token[i:i+n])\n",
    "    return list_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_spacy(path):\n",
    "    pos_ko = [\"NUM\", \"X\", \"SYM\", \"PUNCT\", \"SPACE\"]\n",
    "    str_base = \" \"\n",
    "    list_lemma = []\n",
    "    list_token = []\n",
    "    nombre_tokens = 0\n",
    "    with open(path, encoding=\"utf8\") as file:\n",
    "        text_ivan = file.readlines()\n",
    "        text_ivan_str = str_base.join(text_ivan)\n",
    "        text_ivan_cleaned = clean_text(text_ivan_str.lower())\n",
    "\n",
    "        docs = nlp(text_ivan_cleaned)\n",
    "        nombre_tokens += len(docs)\n",
    "        \n",
    "        for token in docs:\n",
    "            if token.pos_ not in pos_ko:\n",
    "                list_lemma.append(token.lemma_)\n",
    "                list_token.append(token.text)\n",
    "\n",
    "    return list_lemma, list_token, nombre_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_most_common_features = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_name = \"data/Tolstoi - La mort de Ivan Ilitch.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/Tolstoi - La mort de Ivan Ilitch.txt : 25885 tokens\n"
     ]
    }
   ],
   "source": [
    "list_lemma_result, list_bigram_lemma_result, list_trigram_lemma_result, list_token_result, list_bigram_token_result, list_trigram_token_result = get_n_features(path_name, n_most_common_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_freq_token(list_lemma, list_select):\n",
    "    \n",
    "    dict_result = dict.fromkeys(list_select)\n",
    "    \n",
    "    dict_temp = Counter(list_lemma)\n",
    "        \n",
    "    for key in dict_temp.keys():\n",
    "        if key in dict_result.keys():\n",
    "            dict_result[key] = dict_temp[key]/len(list_lemma)\n",
    "    \n",
    "    return dict_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_list(list_selected, list_chunks_tokens, doc_name):\n",
    "    \n",
    "    df_lemma = pd.DataFrame()\n",
    "    dict_chunks = {}\n",
    "    i=1\n",
    "    \n",
    "    for chunk in list_chunks_tokens:\n",
    "                \n",
    "        dict_chunk = dict_freq_token(chunk, list_selected)\n",
    "        dict_chunk[\"index\"] = doc_name+'_chunk_'+str(i)\n",
    "\n",
    "        #update ici pour les autres features\n",
    "        \n",
    "        df_temp_lemma = pd.DataFrame(dict_chunk, index=[0])\n",
    "        \n",
    "        df_lemma = df_lemma.append(df_temp_lemma, ignore_index = True)\n",
    "        \n",
    "        i+=1\n",
    "    return df_lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mouli_ivan(path_name, n, list_lemma_select):\n",
    "    \n",
    "    str_base = \" \"\n",
    "    list_illitch = []\n",
    "    \n",
    "    dict_results_lemma = {}\n",
    "    \n",
    "    df_lemma = pd.DataFrame()\n",
    "    \n",
    "    for doc in glob(path_name):\n",
    "        \n",
    "        doc_name = path.splitext(path.basename(doc))[0]\n",
    "        \n",
    "        list_lemma_temp, list_token_temp, nombre_tokens = pipeline_spacy(doc)\n",
    "        print(doc +' : '+str(nombre_tokens)+' tokens')\n",
    "\n",
    "        list_chunks_lemma = chunkise(list_lemma_temp, n)\n",
    "        \n",
    "        df_lemma_temp = compute_list(list_lemma_select, list_chunks_lemma, doc_name)\n",
    "        \n",
    "        df_lemma = df_lemma.append(df_lemma_temp, ignore_index = True)\n",
    "\n",
    "    df_lemma.set_index(\"index\", inplace = True)\n",
    "        \n",
    "    return df_lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_token_per_chunk = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_name = r'data/chapitres_II/*.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/chapitres_II/chap_10.txt : 965 tokens\n",
      "data/chapitres_II/chap_03.txt : 3155 tokens\n",
      "data/chapitres_II/chap_09.txt : 1213 tokens\n",
      "data/chapitres_II/chap_04.txt : 3186 tokens\n",
      "data/chapitres_II/chap_02.txt : 3314 tokens\n",
      "data/chapitres_II/chap_12.txt : 996 tokens\n",
      "data/chapitres_II/chap_07.txt : 1897 tokens\n",
      "data/chapitres_II/chap_11.txt : 1170 tokens\n",
      "data/chapitres_II/chap_08.txt : 2962 tokens\n",
      "data/chapitres_II/chap_05.txt : 1919 tokens\n",
      "data/chapitres_II/chap_06.txt : 1375 tokens\n",
      "data/chapitres_II/chap_01.txt : 3698 tokens\n"
     ]
    }
   ],
   "source": [
    "df_lemma_ivan = mouli_ivan(path_name, n_token_per_chunk, list_lemma_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>le</th>\n",
       "      <th>de</th>\n",
       "      <th>et</th>\n",
       "      <th>il</th>\n",
       "      <th>son</th>\n",
       "      <th>un</th>\n",
       "      <th>à</th>\n",
       "      <th>lui</th>\n",
       "      <th>se</th>\n",
       "      <th>ce</th>\n",
       "      <th>...</th>\n",
       "      <th>soirée</th>\n",
       "      <th>sapprocher</th>\n",
       "      <th>intérieur</th>\n",
       "      <th>partenaire</th>\n",
       "      <th>tendre</th>\n",
       "      <th>meuble</th>\n",
       "      <th>asseoir</th>\n",
       "      <th>lenfance</th>\n",
       "      <th>calmer</th>\n",
       "      <th>pénibl</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>chap_10_chunk_1</th>\n",
       "      <td>0.10</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.03</td>\n",
       "      <td>None</td>\n",
       "      <td>0.02</td>\n",
       "      <td>None</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.02</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.01</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chap_10_chunk_2</th>\n",
       "      <td>0.11</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.07</td>\n",
       "      <td>None</td>\n",
       "      <td>0.04</td>\n",
       "      <td>None</td>\n",
       "      <td>0.01</td>\n",
       "      <td>None</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chap_10_chunk_3</th>\n",
       "      <td>0.07</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.04</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chap_10_chunk_4</th>\n",
       "      <td>0.11</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.03</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.01</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chap_10_chunk_5</th>\n",
       "      <td>0.05</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.01</td>\n",
       "      <td>None</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.02</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chap_01_chunk_26</th>\n",
       "      <td>0.04</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>None</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.01</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chap_01_chunk_27</th>\n",
       "      <td>0.02</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.04</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chap_01_chunk_28</th>\n",
       "      <td>0.07</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.01</td>\n",
       "      <td>None</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.05</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chap_01_chunk_29</th>\n",
       "      <td>0.09</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.01</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chap_01_chunk_30</th>\n",
       "      <td>0.07</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.01</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>213 rows × 500 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    le    de    et    il   son    un    à   lui    se    ce  \\\n",
       "index                                                                          \n",
       "chap_10_chunk_1   0.10  0.05  0.04  0.03  0.03  None  0.02  None  0.01  0.02   \n",
       "chap_10_chunk_2   0.11  0.03  0.07  None  0.04  None  0.01  None  0.01  0.01   \n",
       "chap_10_chunk_3   0.07  0.11  0.01  0.03  0.02  0.02  0.01  0.01  0.03  0.04   \n",
       "chap_10_chunk_4   0.11  0.07  0.03  0.01  0.03  0.01  0.01  0.02  0.01  0.03   \n",
       "chap_10_chunk_5   0.05  0.07  0.04  0.02  0.03  0.03  0.01  None  0.01  0.02   \n",
       "...                ...   ...   ...   ...   ...   ...   ...   ...   ...   ...   \n",
       "chap_01_chunk_26  0.04  0.07  0.03  0.02  0.02  None  0.06  0.07  0.05  0.01   \n",
       "chap_01_chunk_27  0.02  0.03  0.02  0.03  0.04  0.02  0.02  0.03  0.03  0.04   \n",
       "chap_01_chunk_28  0.07  0.05  0.03  0.01  None  0.05  0.02  0.05  None  None   \n",
       "chap_01_chunk_29  0.09  0.06  0.04  0.01  0.02  0.04  0.04  0.01  None  None   \n",
       "chap_01_chunk_30  0.07  0.06  0.04  0.01  0.03  0.02  0.01  0.01  0.01  None   \n",
       "\n",
       "                  ... soirée sapprocher intérieur partenaire tendre meuble  \\\n",
       "index             ...                                                          \n",
       "chap_10_chunk_1   ...    None       None       0.01       None   None   None   \n",
       "chap_10_chunk_2   ...    None       None       None       None   None   None   \n",
       "chap_10_chunk_3   ...    None       None       None       None   None   None   \n",
       "chap_10_chunk_4   ...    None       None       None       None   None   None   \n",
       "chap_10_chunk_5   ...    None       None       None       None   None   None   \n",
       "...               ...     ...        ...        ...        ...    ...    ...   \n",
       "chap_01_chunk_26  ...    None       None       None       None   None   None   \n",
       "chap_01_chunk_27  ...    None       None       None       None   None   None   \n",
       "chap_01_chunk_28  ...    None       None       None       None   None   None   \n",
       "chap_01_chunk_29  ...    None       None       None       None   None   None   \n",
       "chap_01_chunk_30  ...    None       None       None       None   0.01   None   \n",
       "\n",
       "                 asseoir lenfance calmer pénibl  \n",
       "index                                             \n",
       "chap_10_chunk_1     None     None   None    None  \n",
       "chap_10_chunk_2     None     None   None    None  \n",
       "chap_10_chunk_3     None     None   None    None  \n",
       "chap_10_chunk_4     None     0.01   None    None  \n",
       "chap_10_chunk_5     None     None   None    None  \n",
       "...                  ...      ...    ...     ...  \n",
       "chap_01_chunk_26    None     None   None    0.03  \n",
       "chap_01_chunk_27    None     None   None    None  \n",
       "chap_01_chunk_28    None     None   None    None  \n",
       "chap_01_chunk_29    None     None   None    None  \n",
       "chap_01_chunk_30    None     None   None    None  \n",
       "\n",
       "[213 rows x 500 columns]"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lemma_ivan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import set_config\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.combine import SMOTEENN\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chap(index):\n",
    "    list_chapitre = []\n",
    "    str_base = '_'\n",
    "    for elem in index:\n",
    "        chap = elem.split(\"_\")[0:2]\n",
    "        list_chapitre.append(str_base.join(chap))\n",
    "    return list_chapitre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_chapitre = get_chap(df_lemma_ivan.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lemma_ivan['chapitre']=list_chapitre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def canonizer(data, test_size=0.1, random_state=42, sampling=None, cross_validation=False, cv=5, kernel='rbf', nb_coef=20):\n",
    "    \n",
    "    df_results = pd.DataFrame()\n",
    "    \n",
    "    if cross_validation == True:\n",
    "        pipe = make_pipeline(StandardScaler(), SVC(kernel=kernel, probability=True))\n",
    "        cv_results = cross_validate(pipe, data.drop(['chapitre'], axis=1), data['chapitre'], cv=cv)\n",
    "        return pipe, cv_results\n",
    "    \n",
    "    else:\n",
    "        if kernel == 'rbf':\n",
    "            pipe = make_pipeline(StandardScaler(), Normalizer(), SVC(kernel=kernel, probability=True))\n",
    "        else:\n",
    "            pipe = make_pipeline(StandardScaler(), SVC(kernel=kernel, probability=True))\n",
    "    \n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(data.drop(['chapitre'], axis=1), data['chapitre'], test_size=test_size, random_state=random_state)\n",
    "        print('Original dataset shape {}'.format(Counter(y_train)))\n",
    "                \n",
    "        if sampling is not None:     \n",
    "            if sampling == 'over':\n",
    "                ros = RandomOverSampler(random_state=10)\n",
    "                X_resampled, y_resampled = ros.fit_resample(X_train, y_train)\n",
    "                \n",
    "            elif sampling == 'under':\n",
    "                rus = RandomUnderSampler(random_state=10)\n",
    "                X_resampled, y_resampled = rus.fit_resample(X_train, y_train)\n",
    "                \n",
    "            elif sampling == \"smoteenn\":\n",
    "                smote_enn = SMOTEENN(random_state=10)\n",
    "                X_resampled, y_resampled = smote_enn.fit_resample(X_train, y_train)\n",
    "                \n",
    "            elif sampling == 'smotetomek':\n",
    "                smote_tomek = SMOTETomek(random_state=10)\n",
    "                X_resampled, y_resampled = smote_tomek.fit_resample(X_train, y_train)\n",
    "            \n",
    "            else:\n",
    "                print('Please follow the sampling possible values : over, under, smoteenn, smotetomek')\n",
    "                return\n",
    "                \n",
    "            print('Resampled dataset shape {}'.format(Counter(y_resampled)))\n",
    "        \n",
    "            pipe.fit(X_resampled, y_resampled)\n",
    "        \n",
    "        else:\n",
    "            pipe.fit(X_train, y_train)\n",
    "        print(metrics.classification_report(y_test, pipe.predict(X_test)))\n",
    "    \n",
    "        if kernel == 'linear':\n",
    "            coefs = pipe.named_steps['svc'].coef_\n",
    "            return pipe, coefs\n",
    "            #plot_coefficients(*coefs, data.columns, nb_coef)\n",
    "            \n",
    "        #df_results['metadata'] = y_test\n",
    "        #df_results['proba canon'] = pipe.predict_proba(X_test)[:,0]\n",
    "        #df_results['proba non-canon'] = pipe.predict_proba(X_test)[:,1]\n",
    "        #df_results['prediction']= pipe.predict(X_test)\n",
    "    \n",
    "        #df_results['accord'] = [True if row['metadata'] == row['prediction'] else False for index, row in df_results.iterrows()]\n",
    "    \n",
    "    return pipe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lemma_ivan = df_lemma_ivan.replace(np.nan, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset shape Counter({'chap_01': 25, 'chap_04': 25, 'chap_02': 25, 'chap_03': 24, 'chap_08': 22, 'chap_07': 14, 'chap_05': 13, 'chap_06': 9, 'chap_11': 9, 'chap_09': 9, 'chap_12': 8, 'chap_10': 8})\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     chap_01       0.67      0.80      0.73         5\n",
      "     chap_02       0.80      1.00      0.89         4\n",
      "     chap_03       1.00      0.67      0.80         3\n",
      "     chap_04       0.22      1.00      0.36         2\n",
      "     chap_05       0.00      0.00      0.00         2\n",
      "     chap_06       0.00      0.00      0.00         2\n",
      "     chap_07       0.00      0.00      0.00         2\n",
      "     chap_08       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.55        22\n",
      "   macro avg       0.34      0.43      0.35        22\n",
      "weighted avg       0.45      0.55      0.47        22\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/humanum/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/humanum/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/humanum/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "pipe_test = canonizer(df_lemma_ivan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset shape Counter({'chap_01': 25, 'chap_04': 25, 'chap_02': 25, 'chap_03': 24, 'chap_08': 22, 'chap_07': 14, 'chap_05': 13, 'chap_06': 9, 'chap_11': 9, 'chap_09': 9, 'chap_12': 8, 'chap_10': 8})\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     chap_01       1.00      0.80      0.89         5\n",
      "     chap_02       0.75      0.75      0.75         4\n",
      "     chap_03       0.67      0.67      0.67         3\n",
      "     chap_04       0.33      1.00      0.50         2\n",
      "     chap_05       1.00      1.00      1.00         2\n",
      "     chap_06       0.00      0.00      0.00         2\n",
      "     chap_07       1.00      0.50      0.67         2\n",
      "     chap_08       0.50      0.50      0.50         2\n",
      "\n",
      "    accuracy                           0.68        22\n",
      "   macro avg       0.66      0.65      0.62        22\n",
      "weighted avg       0.71      0.68      0.67        22\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/humanum/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/humanum/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/humanum/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "pipe_test, coefs = canonizer(df_lemma_ivan, kernel='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion :\n",
    "\n",
    "- get more features -> test at least 200 bigrams -> extend n features (only mots outils)\n",
    "- merge labels chapitres from qualitative insights\n",
    "- merge labels chapitres from quantitative insights -> Topic Modeling\n",
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
